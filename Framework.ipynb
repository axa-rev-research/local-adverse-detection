{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import pandas\n",
    "import numpy\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.metrics import euclidean_distances\n",
    "\n",
    "\"\"\"\n",
    "This tutorial shows how to generate adversarial examples\n",
    "using FGSM in black-box setting.\n",
    "The original paper can be found at:\n",
    "https://arxiv.org/abs/1602.02697\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "from six.moves import xrange\n",
    "\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.platform import flags\n",
    "\n",
    "from cleverhans.utils_mnist import data_mnist\n",
    "from cleverhans.utils import to_categorical\n",
    "from cleverhans.utils import set_log_level\n",
    "from cleverhans.utils_tf import model_train, model_eval, batch_eval\n",
    "from cleverhans.attacks import FastGradientMethod\n",
    "from cleverhans.attacks_tf import jacobian_graph, jacobian_augmentation\n",
    "\n",
    "from cleverhans_tutorials.tutorial_models import make_basic_cnn, MLP\n",
    "from cleverhans_tutorials.tutorial_models import Flatten, Linear, ReLU, Softmax\n",
    "from cleverhans.utils import TemporaryLogLevel\n",
    "\n",
    "from lad import lad_Thibault as lad\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "MOONS\n",
    "'''\n",
    "def get_moon():\n",
    "    X, y = make_moons(noise=0.3, random_state=1, n_samples=10000)\n",
    "    y2 = numpy.zeros((X.shape[0],2))\n",
    "    for k in range(len(y)):\n",
    "        y2[k][y[k]] = 1\n",
    "    return X, y2\n",
    "\n",
    "def get_german():\n",
    "    path_dataset='data/germancredit.csv'\n",
    "    X = pandas.read_csv(path_dataset, delimiter=\",\", index_col=0)\n",
    "    y = X.label\n",
    "    y = y - 1\n",
    "    X = X.iloc[:,X.columns != 'label']\n",
    "    X = (X-X.mean())/X.std()\n",
    "    y2 = numpy.zeros((X.shape[0],2)) #2=  nb de classes\n",
    "    for k in range(len(y)):\n",
    "        y2[k][y[k]] = 1\n",
    "    return numpy.array(X), numpy.array(y2)\n",
    "\n",
    "DATASETS_ = {'moons':get_moon,\n",
    "            'german': get_german}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a black-box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "PAPERNOT BB\n",
    "'''\n",
    "def Papernot_bbox(sess, x, y, X_train, Y_train, X_test, Y_test,\n",
    "              nb_epochs, batch_size, learning_rate,\n",
    "              rng):\n",
    "    \"\"\"\n",
    "    Define and train a model that simulates the \"remote\"\n",
    "    black-box oracle described in the original paper.\n",
    "    :param sess: the TF session\n",
    "    :param x: the input placeholder for MNIST\n",
    "    :param y: the ouput placeholder for MNIST\n",
    "    :param X_train: the training data for the oracle\n",
    "    :param Y_train: the training labels for the oracle\n",
    "    :param X_test: the testing data for the oracle\n",
    "    :param Y_test: the testing labels for the oracle\n",
    "    :param nb_epochs: number of epochs to train model\n",
    "    :param batch_size: size of training batches\n",
    "    :param learning_rate: learning rate for training\n",
    "    :param rng: numpy.random.RandomState\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # Define TF model graph (for the black-box model)\n",
    "    model = make_basic_cnn()\n",
    "    predictions = model(x)\n",
    "    print(\"Defined TensorFlow model graph.\")\n",
    "\n",
    "    # Train an MNIST model\n",
    "    train_params = {\n",
    "        'nb_epochs': nb_epochs,\n",
    "        'batch_size': batch_size,\n",
    "        'learning_rate': learning_rate\n",
    "    }\n",
    "    model_train(sess, x, y, predictions, X_train, Y_train,\n",
    "                args=train_params, rng=rng)\n",
    "\n",
    "    # Print out the accuracy on legitimate data\n",
    "    eval_params = {'batch_size': batch_size}\n",
    "    accuracy = model_eval(sess, x, y, predictions, X_test, Y_test,\n",
    "                          args=eval_params)\n",
    "    print('Test accuracy of black-box on legitimate test '\n",
    "          'examples: ' + str(accuracy))\n",
    "\n",
    "    return model, predictions, accuracy\n",
    "\n",
    "def RF_bbox(X_train, Y_train, X_test, Y_test):\n",
    "    # Define RF model graph (for the black-box model)\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators=100, n_jobs=-1).fit(X_train, Y_train)\n",
    "    \n",
    "    # Print out the accuracy on legitimate data\n",
    "    #predictions = model.predict_proba(X_test)[1] TEST CHANGER PREDICTIONS > FONCTION\n",
    "    predictions=lambda x: model.predict_proba(x)[1] #predict_proba required ou alors changer du code (argmax et compagnie) de papernot\n",
    "    \n",
    "    accuracy = accuracy_score(Y_test, model.predict(X_test))\n",
    "    #roc_auc = roc_auc_score(Y_test, predictions[1][:,1])\n",
    "    print('Test accuracy of black-box on legitimate test '\n",
    "          'examples: ' + str(accuracy))\n",
    "    #print('Test ROC AUC of black-box on legitimate test ' 'examples: ' + str(roc_auc))\n",
    "        \n",
    "    \n",
    "    return model, predictions, accuracy\n",
    "    \n",
    "BB_MODELS_ = {'dnn': Papernot_bbox,\n",
    "            'rf': RF_bbox}\n",
    "#ne pas utiliser dnn ca marche pas pour le moment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Papernot Surrogate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_tutorial():\n",
    "    \"\"\"\n",
    "    Helper function to check correct configuration of tf for tutorial\n",
    "    :return: True if setup checks completed\n",
    "    \"\"\"\n",
    "\n",
    "    # Set TF random seed to improve reproducibility\n",
    "    tf.set_random_seed(1234)\n",
    "\n",
    "    return True\n",
    "def substitute_model(img_rows=1, img_cols=2, nb_classes=2):\n",
    "    \"\"\"\n",
    "    Defines the model architecture to be used by the substitute. Use\n",
    "    the example model interface.\n",
    "    :param img_rows: number of rows in input\n",
    "    :param img_cols: number of columns in input\n",
    "    :param nb_classes: number of classes in output\n",
    "    :return: tensorflow model\n",
    "    \"\"\"\n",
    "    input_shape = (None, img_rows, img_cols, 1) #on garde format d'origine parce qu'on comprend pas grand chose mais on change valeurs\n",
    "\n",
    "    # Define a fully connected model (it's different than the black-box)\n",
    "    '''layers2 = [Flatten(),\n",
    "              Linear(200),\n",
    "              ReLU(),\n",
    "              Linear(200),\n",
    "              ReLU(),\n",
    "              Linear(nb_classes),\n",
    "              Softmax()]'''\n",
    "    layers1 = [Flatten(), Linear(nb_classes), Softmax()] #surrogate simplifié\n",
    "\n",
    "    return MLP(layers1, input_shape)\n",
    "\n",
    "\n",
    "def train_sub(sess, x, y, bb_model, X_sub, Y_sub, nb_classes,\n",
    "              nb_epochs_s, batch_size, learning_rate, data_aug, lmbda,\n",
    "              rng):\n",
    "    \"\"\"\n",
    "    This function creates the substitute by alternatively\n",
    "    augmenting the training data and training the substitute.\n",
    "    :param sess: TF session\n",
    "    :param x: input TF placeholder\n",
    "    :param y: output TF placeholder\n",
    "    :param bbox_preds: output of black-box model predictions\n",
    "    :param X_sub: initial substitute training data\n",
    "    :param Y_sub: initial substitute training labels\n",
    "    :param nb_classes: number of output classes\n",
    "    :param nb_epochs_s: number of epochs to train substitute model\n",
    "    :param batch_size: size of training batches\n",
    "    :param learning_rate: learning rate for training\n",
    "    :param data_aug: number of times substitute training data is augmented\n",
    "    :param lmbda: lambda from arxiv.org/abs/1602.02697\n",
    "    :param rng: numpy.random.RandomState instance\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Define TF model graph (for the black-box model)\n",
    "    model_sub = substitute_model(img_cols=X_sub.shape[1])\n",
    "    preds_sub = model_sub(x)\n",
    "    print(\"Defined TensorFlow model graph for the substitute.\")\n",
    "\n",
    "    # Define the Jacobian symbolically using TensorFlow\n",
    "    grads = jacobian_graph(preds_sub, x, nb_classes)\n",
    "    # Train the substitute and augment dataset alternatively\n",
    "    for rho in xrange(data_aug):\n",
    "        print(\"Substitute training epoch #\" + str(rho))\n",
    "        train_params = {\n",
    "            'nb_epochs': nb_epochs_s,\n",
    "            'batch_size': batch_size,\n",
    "            'learning_rate': learning_rate\n",
    "        }\n",
    "        with TemporaryLogLevel(logging.WARNING, \"cleverhans.utils.tf\"):\n",
    "            model_train(sess, x, y, preds_sub, X_sub,\n",
    "                        to_categorical(Y_sub, nb_classes),\n",
    "                        init_all=False, args=train_params, rng=rng)\n",
    "\n",
    "        # If we are not at last substitute training iteration, augment dataset\n",
    "        if rho < data_aug - 1:\n",
    "            print(\"Augmenting substitute training data.\")\n",
    "            # Perform the Jacobian augmentation\n",
    "            lmbda_coef = 2 * int(int(rho / 3) != 0) - 1\n",
    "            X_sub = jacobian_augmentation(sess, x, X_sub, Y_sub, grads,\n",
    "                                          lmbda_coef * lmbda)\n",
    "            \n",
    "            print(\"Labeling substitute training data.\")\n",
    "            # Label the newly generated synthetic points using the black-box\n",
    "            Y_sub = numpy.hstack([Y_sub, Y_sub])\n",
    "            X_sub_prev = X_sub[int(len(X_sub)/2):] #on a double le dataset donc prev = ce qu'il y a de nouveau = la moitie\n",
    "            eval_params = {'batch_size': batch_size}\n",
    "            \n",
    "            #bbox_preds = tf.convert_to_tensor(bbox_preds, dtype=tf.float32) TEST CHANGER PREDICTIONS > FONCTION           \n",
    "            #bbox_val = batch_eval2(sess, [x], [bbox_preds], [X_sub_prev], args=eval_params)[0] TEST CHANGER PREDICTIONS > FONCTION\n",
    "            \n",
    "            #bbox_val = bbox_preds(X_sub_prev) #normalement batch eval sert juste à sortir les preds...?\n",
    "            bbox_val = bb_model.predict(X_sub_prev)\n",
    "            # Note here that we take the argmax because the adversary\n",
    "            # only has access to the label (not the probabilities) output\n",
    "            # by the black-box model\n",
    "            Y_sub[int(len(X_sub)/2):] = numpy.argmax(bbox_val, axis=1)\n",
    "    return model_sub, preds_sub "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usage: \n",
    "print(\"Training the substitute model.\")\n",
    "    train_sub_out = train_sub(sess, x, y, bbox_preds, X_sub, Y_sub,\n",
    "                              nb_classes, nb_epochs_s, batch_size,\n",
    "                              learning_rate, data_aug, lmbda, rng=rng)\n",
    "    model_sub, preds_sub = train_sub_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our surrogate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_points_hypersphere(x_center, radius_, n_points_):\n",
    "\n",
    "        res = []\n",
    "        while len(res) < n_points_:\n",
    "        \n",
    "            n_points_left_ = n_points_ - len(res)\n",
    "            # About half the points are lost in the test hypercube => hypersphere\n",
    "            lbound = numpy.repeat([x_center.values-(radius_/2.)], n_points_left_*2, axis=0)\n",
    "            hbound = numpy.repeat([x_center.values+(radius_/2.)], n_points_left_*2, axis=0)\n",
    "            points = numpy.random.uniform(low=lbound, high=hbound)\n",
    "            # Check if x_generated is within hypersphere (if kind=='hypersphere')\n",
    "            for x_generated in points:\n",
    "                if euclidean(x_generated, x_center.values) < radius_:\n",
    "                    res.append(x_generated)\n",
    "                if len(res) == n_points_:\n",
    "                    break\n",
    "\n",
    "        return pandas.DataFrame(numpy.array(res))\n",
    "    \n",
    "def generate_inside_ball(center, segment=(0,1), n=1): #verifier algo comprendre racine 1/d et rapport entre segment et radius\n",
    "    d = center.shape[0]\n",
    "    z = numpy.random.normal(0, 1, (n, d))\n",
    "    z = numpy.array([a * b / c for a, b, c in zip(z, numpy.random.uniform(*segment, n),  norm(z))])\n",
    "    z = z + center\n",
    "    return z \n",
    "def norm(v):\n",
    "        return numpy.linalg.norm(v, ord=2, axis=1) #array of l2 norms of vectors in v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training black box on 800 examples\n",
      "Testing black box and substitute on 150  examples\n",
      "Using  50  examples to start PP substitute\n",
      "Preparing the black-box model.\n",
      "Test accuracy of black-box on legitimate test examples: 0.7466666666666667\n",
      "Training the Pépèrenot substitute model.\n",
      "Defined TensorFlow model graph for the substitute.\n",
      "Substitute training epoch #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 2018-06-28 10:50:19,199 cleverhans] Epoch 0 took 0.038666725158691406 seconds\n",
      "[INFO 2018-06-28 10:50:19,200 cleverhans] Epoch 1 took 0.0010492801666259766 seconds\n",
      "[INFO 2018-06-28 10:50:19,202 cleverhans] Epoch 2 took 0.0012369155883789062 seconds\n",
      "[INFO 2018-06-28 10:50:19,203 cleverhans] Epoch 3 took 0.0010046958923339844 seconds\n",
      "[INFO 2018-06-28 10:50:19,205 cleverhans] Epoch 4 took 0.0009539127349853516 seconds\n",
      "[INFO 2018-06-28 10:50:19,206 cleverhans] Epoch 5 took 0.0009350776672363281 seconds\n",
      "[INFO 2018-06-28 10:50:19,208 cleverhans] Epoch 6 took 0.0012059211730957031 seconds\n",
      "[INFO 2018-06-28 10:50:19,209 cleverhans] Epoch 7 took 0.0012526512145996094 seconds\n",
      "[INFO 2018-06-28 10:50:19,211 cleverhans] Epoch 8 took 0.0010731220245361328 seconds\n",
      "[INFO 2018-06-28 10:50:19,212 cleverhans] Epoch 9 took 0.0010924339294433594 seconds\n",
      "[INFO 2018-06-28 10:50:19,213 cleverhans] Completed model training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting substitute training data.\n",
      "Labeling substitute training data.\n",
      "Substitute training epoch #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 2018-06-28 10:50:28,218 cleverhans] Epoch 0 took 0.040926218032836914 seconds\n",
      "[INFO 2018-06-28 10:50:28,220 cleverhans] Epoch 1 took 0.0022416114807128906 seconds\n",
      "[INFO 2018-06-28 10:50:28,223 cleverhans] Epoch 2 took 0.0021660327911376953 seconds\n",
      "[INFO 2018-06-28 10:50:28,226 cleverhans] Epoch 3 took 0.002201080322265625 seconds\n",
      "[INFO 2018-06-28 10:50:28,229 cleverhans] Epoch 4 took 0.0023186206817626953 seconds\n",
      "[INFO 2018-06-28 10:50:28,232 cleverhans] Epoch 5 took 0.002168893814086914 seconds\n",
      "[INFO 2018-06-28 10:50:28,234 cleverhans] Epoch 6 took 0.0018134117126464844 seconds\n",
      "[INFO 2018-06-28 10:50:28,236 cleverhans] Epoch 7 took 0.0016188621520996094 seconds\n",
      "[INFO 2018-06-28 10:50:28,238 cleverhans] Epoch 8 took 0.0017931461334228516 seconds\n",
      "[INFO 2018-06-28 10:50:28,240 cleverhans] Epoch 9 took 0.0016345977783203125 seconds\n",
      "[INFO 2018-06-28 10:50:28,241 cleverhans] Completed model training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting substitute training data.\n",
      "Labeling substitute training data.\n",
      "Substitute training epoch #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 2018-06-28 10:50:46,400 cleverhans] Epoch 0 took 0.04196429252624512 seconds\n",
      "[INFO 2018-06-28 10:50:46,404 cleverhans] Epoch 1 took 0.0037164688110351562 seconds\n",
      "[INFO 2018-06-28 10:50:46,408 cleverhans] Epoch 2 took 0.0036008358001708984 seconds\n",
      "[INFO 2018-06-28 10:50:46,412 cleverhans] Epoch 3 took 0.003598451614379883 seconds\n",
      "[INFO 2018-06-28 10:50:46,417 cleverhans] Epoch 4 took 0.0036733150482177734 seconds\n",
      "[INFO 2018-06-28 10:50:46,421 cleverhans] Epoch 5 took 0.0037093162536621094 seconds\n",
      "[INFO 2018-06-28 10:50:46,425 cleverhans] Epoch 6 took 0.0037338733673095703 seconds\n",
      "[INFO 2018-06-28 10:50:46,428 cleverhans] Epoch 7 took 0.0029418468475341797 seconds\n",
      "[INFO 2018-06-28 10:50:46,432 cleverhans] Epoch 8 took 0.003054380416870117 seconds\n",
      "[INFO 2018-06-28 10:50:46,436 cleverhans] Epoch 9 took 0.0037856101989746094 seconds\n",
      "[INFO 2018-06-28 10:50:46,437 cleverhans] Completed model training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting substitute training data.\n",
      "Labeling substitute training data.\n",
      "Substitute training epoch #3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 2018-06-28 10:51:24,144 cleverhans] Epoch 0 took 0.044950246810913086 seconds\n",
      "[INFO 2018-06-28 10:51:24,151 cleverhans] Epoch 1 took 0.006299495697021484 seconds\n",
      "[INFO 2018-06-28 10:51:24,158 cleverhans] Epoch 2 took 0.006520271301269531 seconds\n",
      "[INFO 2018-06-28 10:51:24,167 cleverhans] Epoch 3 took 0.008724451065063477 seconds\n",
      "[INFO 2018-06-28 10:51:24,178 cleverhans] Epoch 4 took 0.010510921478271484 seconds\n",
      "[INFO 2018-06-28 10:51:24,191 cleverhans] Epoch 5 took 0.011692047119140625 seconds\n",
      "[INFO 2018-06-28 10:51:24,204 cleverhans] Epoch 6 took 0.012732505798339844 seconds\n",
      "[INFO 2018-06-28 10:51:24,218 cleverhans] Epoch 7 took 0.01323699951171875 seconds\n",
      "[INFO 2018-06-28 10:51:24,229 cleverhans] Epoch 8 took 0.010494470596313477 seconds\n",
      "[INFO 2018-06-28 10:51:24,240 cleverhans] Epoch 9 took 0.010268449783325195 seconds\n",
      "[INFO 2018-06-28 10:51:24,240 cleverhans] Completed model training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting substitute training data.\n",
      "Labeling substitute training data.\n",
      "Substitute training epoch #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 2018-06-28 10:52:48,197 cleverhans] Epoch 0 took 0.053496360778808594 seconds\n",
      "[INFO 2018-06-28 10:52:48,214 cleverhans] Epoch 1 took 0.01669788360595703 seconds\n",
      "[INFO 2018-06-28 10:52:48,228 cleverhans] Epoch 2 took 0.013033628463745117 seconds\n",
      "[INFO 2018-06-28 10:52:48,242 cleverhans] Epoch 3 took 0.013918876647949219 seconds\n",
      "[INFO 2018-06-28 10:52:48,258 cleverhans] Epoch 4 took 0.015115737915039062 seconds\n",
      "[INFO 2018-06-28 10:52:48,276 cleverhans] Epoch 5 took 0.01751565933227539 seconds\n",
      "[INFO 2018-06-28 10:52:48,290 cleverhans] Epoch 6 took 0.013493061065673828 seconds\n",
      "[INFO 2018-06-28 10:52:48,306 cleverhans] Epoch 7 took 0.01522064208984375 seconds\n",
      "[INFO 2018-06-28 10:52:48,323 cleverhans] Epoch 8 took 0.016368389129638672 seconds\n",
      "[INFO 2018-06-28 10:52:48,337 cleverhans] Epoch 9 took 0.013483524322509766 seconds\n",
      "[INFO 2018-06-28 10:52:48,338 cleverhans] Completed model training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting substitute training data.\n",
      "Labeling substitute training data.\n",
      "Substitute training epoch #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 2018-06-28 10:56:01,138 cleverhans] Epoch 0 took 0.07132744789123535 seconds\n",
      "[INFO 2018-06-28 10:56:01,168 cleverhans] Epoch 1 took 0.029441118240356445 seconds\n",
      "[INFO 2018-06-28 10:56:01,199 cleverhans] Epoch 2 took 0.030104398727416992 seconds\n",
      "[INFO 2018-06-28 10:56:01,234 cleverhans] Epoch 3 took 0.034760236740112305 seconds\n",
      "[INFO 2018-06-28 10:56:01,268 cleverhans] Epoch 4 took 0.03221464157104492 seconds\n",
      "[INFO 2018-06-28 10:56:01,298 cleverhans] Epoch 5 took 0.029929637908935547 seconds\n",
      "[INFO 2018-06-28 10:56:01,330 cleverhans] Epoch 6 took 0.030553340911865234 seconds\n",
      "[INFO 2018-06-28 10:56:01,365 cleverhans] Epoch 7 took 0.03480076789855957 seconds\n",
      "[INFO 2018-06-28 10:56:01,398 cleverhans] Epoch 8 took 0.032805442810058594 seconds\n",
      "[INFO 2018-06-28 10:56:01,430 cleverhans] Epoch 9 took 0.030975818634033203 seconds\n",
      "[INFO 2018-06-28 10:56:01,431 cleverhans] Completed model training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Local Surrogate substitute model.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'lad' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-58acad9a29d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0mlmbda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m \u001b[0;31m# params exploration pour augmentation data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0mradius_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;31m# NEW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m \u001b[0mmain_fidelity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mradius_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-58acad9a29d5>\u001b[0m in \u001b[0;36mmain_fidelity\u001b[0;34m(radius)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbb_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mbb_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_sub_ls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLocalSurrogate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblackbox\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbb_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_support_points\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_local_surrogate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_toexplain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0;31m#ls_acc = accuracy_score(train_sub_ls.predict(X_test), Y_test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mls_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_sub_ls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msupport_x_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbb_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msupport_x_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lad' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def main_fidelity(radius):\n",
    "    accuracies = {}\n",
    "    fidelities = {}\n",
    "    \n",
    "    \n",
    "    # Seed random number generator so tutorial is reproducible\n",
    "    rng = numpy.random.RandomState([2017, 8, 30])\n",
    "\n",
    "    # Thibault: Tensorflow stuff\n",
    "    set_log_level(logging.DEBUG)\n",
    "    assert setup_tutorial()\n",
    "    sess = tf.Session()\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Data\n",
    "    X, Y = DATASETS_['german']()\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20)\n",
    "    X_sub = X_test[:holdout]\n",
    "    Y_sub = numpy.argmax(Y_test[:holdout], axis=1)\n",
    "\n",
    "    ## Redefine test set as remaining samples unavailable to adversaries\n",
    "    ### N.B Thibault: c'est pour le substitute de Papernot\n",
    "    X_test = X_test[holdout:]\n",
    "    Y_test = Y_test[holdout:]\n",
    "    print(\"Training black box on\",X_train.shape[0], \"examples\")\n",
    "    print('Testing black box and substitute on', X_test.shape[0],' examples')\n",
    "    print(\"Using \", holdout, \" examples to start PP substitute\")\n",
    "    ## Define input and output TF placeholders\n",
    "    ### N.B. Thibault: restes de Tensorflow, utilisé pour le substitute de Papernot...\n",
    "    x = tf.placeholder(tf.float32, shape=(None, 20))\n",
    "    y = tf.placeholder(tf.float32, shape=(None, 2))  \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Instance to explain\n",
    "    x_toexplain = pandas.Series(X_test[0]).copy()\n",
    "    support_x_ = numpy.array(get_random_points_hypersphere(x_toexplain, radius_=radius, n_points_=1000))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Simulate the black-box model\n",
    "    print(\"Preparing the black-box model.\")\n",
    "    prep_bbox_out = BB_MODELS_['rf'](X_train, Y_train, X_test, Y_test)\n",
    "    bb_model, bbox_preds, accuracies['bbox'] = prep_bbox_out #bbox_preds fonction predict\n",
    "    \n",
    "    # Train PAPERNOT substitute\n",
    "    print(\"Training the Pépèrenot substitute model.\")\n",
    "    train_sub_pap = train_sub(sess, x, y, bb_model, X_sub, Y_sub,\n",
    "                              nb_classes, nb_epochs_s, batch_size,\n",
    "                              learning_rate, data_aug, lmbda, rng=rng)\n",
    "    model_sub, preds_sub = train_sub_pap\n",
    "    \n",
    "    #feed_dict = {x:support_x_, y:bbox_preds(support_x_)}\n",
    "    \n",
    "    eval_params = {'batch_size': batch_size}\n",
    "    pap_acc = model_eval(sess, x, y, preds_sub, X_test, Y_test, args=eval_params) \n",
    "    pap_fid = model_eval(sess, x, y, preds_sub, support_x_, bb_model.predict(support_x_) , args=eval_params)\n",
    "    accuracies['papernot'] = pap_acc\n",
    "    fidelities['papernot'] = pap_fid\n",
    "    \n",
    "    \n",
    "    # Train OUR subtitute\n",
    "    print(\"Training Local Surrogate substitute model.\")\n",
    "    pred = bb_model.predict\n",
    "    bb_model.predict = lambda x: pred(x)[:,1]\n",
    "    _, train_sub_ls = lad.LocalSurrogate(pandas.DataFrame(X), blackbox=bb_model, n_support_points=100, max_depth=3).get_local_surrogate(x_toexplain)\n",
    "    #ls_acc = accuracy_score(train_sub_ls.predict(X_test), Y_test)\n",
    "    ls_fid = accuracy_score(train_sub_ls.predict(support_x_), bb_model.predict(support_x_))\n",
    "    #accuracies['localsurrogate'] = ls_acc\n",
    "    fidelities['localsurrogate'] = ls_fid\n",
    "    '''\n",
    "\n",
    "    \n",
    "    '''# Initialize the Fast Gradient Sign Method (FGSM) attack object.\n",
    "    fgsm_par = {'eps': 0.5, 'ord': numpy.inf, 'clip_min': 0., 'clip_max': 1.} #ord: norme L1, l2 ou linfini\n",
    "    fgsm = FastGradientMethod(model_sub, sess=sess)\n",
    "\n",
    "    # Craft adversarial examples using the substitute\n",
    "    eval_params = {'batch_size': batch_size}\n",
    "    x_adv_sub = fgsm.generate(x, **fgsm_par)\n",
    "\n",
    "    # Evaluate the accuracy of the \"black-box\" model on adversarial examples\n",
    "    accuracy = accuracy_score(Y_test, bb_model.predict(sess.run(x_adv_sub, feed_dict={x: X_test})))\n",
    "    #model_eval(sess, x, y, bb_model.predict(x_adv_sub), X_test, Y_test,\n",
    "    #                      args=eval_params)\n",
    "    print('Test accuracy of oracle on adversarial examples generated '\n",
    "          'using the substitute: ' + str(accuracy))\n",
    "    accuracies['bbox_on_sub_adv_ex'] = accuracy\n",
    "    \n",
    "    return fidelities, accuracies\n",
    "\n",
    "\n",
    "\n",
    "nb_classes=2 #\n",
    "batch_size=20 #\n",
    "learning_rate=0.001 #\n",
    "nb_epochs=0 # Nombre d'itération bbox osef\n",
    "holdout=50 # Nombre d'exemples utilisés au début pour générer data (Pap-substitute)\n",
    "data_aug=6 # Nombre d'itérations d'augmentation du dataset {IMPORTANT pour Pap-substitute}\n",
    "nb_epochs_s=10 # Nombre d'itérations pour train substitute\n",
    "lmbda=0.1 # params exploration pour augmentation data\n",
    "radius_ = 0.5 # NEW\n",
    "main_fidelity(radius_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Il faut trouver une facon de faire la boucle\n",
    "\n",
    "pour radius:\n",
    "    genere black box\n",
    "    genere surrogate papernot\n",
    "    \n",
    "    pour observation dans test:\n",
    "        genere local surrogate\n",
    "        evalue papernot local\n",
    "        evalue local surrogate local\n",
    "outputs:\n",
    "papernot: {radius: [accuracy locale de chaque point}\n",
    "pareil pour ls}\n",
    "\n",
    "\n",
    "TODO: check histoire de boucle radius comment ca se goupille\n",
    "voir si ca tourne\n",
    "faire graphe...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'azeazeazer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-93bb202bf5b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mazeazeazer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# Seed random number generator so tutorial is reproducible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mrng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2017\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Thibault: Tensorflow stuff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'azeazeazer' is not defined"
     ]
    }
   ],
   "source": [
    "azeazeazer\n",
    "# Seed random number generator so tutorial is reproducible\n",
    "rng = numpy.random.RandomState([2017, 8, 30])\n",
    "\n",
    "# Thibault: Tensorflow stuff\n",
    "set_log_level(logging.DEBUG)\n",
    "assert setup_tutorial()\n",
    "sess = tf.Session()\n",
    "\n",
    "\n",
    "\n",
    "# Data\n",
    "X, Y = DATASETS_['german']()\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.30)\n",
    "X_sub = X_test[:holdout]\n",
    "Y_sub = numpy.argmax(Y_test[:holdout], axis=1)\n",
    "\n",
    "## Redefine test set as remaining samples unavailable to adversaries\n",
    "### N.B Thibault: c'est pour le substitute de Papernot\n",
    "X_test = X_test[holdout:]\n",
    "Y_test = Y_test[holdout:]\n",
    "print(\"Training black box on\",X_train.shape[0], \"examples\")\n",
    "print('Testing black box and substitute on', X_test.shape[0],' examples')\n",
    "print(\"Using \", holdout, \" examples to start PP substitute\")\n",
    "## Define input and output TF placeholders\n",
    "### N.B. Thibault: restes de Tensorflow, utilisé pour le substitute de Papernot...\n",
    "x = tf.placeholder(tf.float32, shape=(None, X.shape[1]))\n",
    "y = tf.placeholder(tf.float32, shape=(None, Y.shape[1]))  \n",
    "\n",
    "# Simulate the black-box model\n",
    "print(\"Preparing the black-box model.\")\n",
    "prep_bbox_out = BB_MODELS_['rf'](X_train, Y_train, X_test, Y_test)\n",
    "bb_model, bbox_preds, _ = prep_bbox_out #bbox_preds fonction predict\n",
    "\n",
    "# Train PAPERNOT substitute\n",
    "print(\"Training the Pépèrenot substitute model.\")\n",
    "train_sub_pap = train_sub(sess, x, y, bb_model, X_sub, Y_sub,\n",
    "                          nb_classes, nb_epochs_s, batch_size,\n",
    "                          learning_rate, data_aug, lmbda, rng=rng)\n",
    "model_sub, preds_sub = train_sub_pap\n",
    "\n",
    "eval_params = {'batch_size': batch_size}\n",
    "pap_acc = model_eval(sess, x, y, preds_sub, X_test, Y_test, args=eval_params) \n",
    "print(pap_acc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "\n",
    "def pred(x):\n",
    "        return bb_model.predict(x)[:,1]\n",
    "\n",
    "xs_toexplain = [pandas.Series(xi) for xi in X_test[:1000,:]]\n",
    "radius_perc=[0.05,0.1,0.2,0.3,0.4,0.5]#,0.6,0.7,0.8,0.9,1] \n",
    "papernot = {}\n",
    "localsurr = {}\n",
    "papernot = dict([(r, []) for r in radius_perc])\n",
    "localsurrogate = dict([(r, []) for r in radius_perc])\n",
    "c = 0\n",
    "\n",
    "\n",
    "\n",
    "for x_toexplain in xs_toexplain:\n",
    "    c += 1\n",
    "    if c % 100 == 0:\n",
    "        print('iter', c)\n",
    "    \n",
    "    print(\"Training Local Surrogate substitute model.\")\n",
    "    \n",
    "    \n",
    "    _, train_sub_ls = lad.LocalSurrogate(pandas.DataFrame(X), blackbox=bb_model, n_support_points=100, max_depth=3).get_local_surrogate(x_toexplain)\n",
    "    \n",
    "    print(\"Calculating distances.\")\n",
    "    dists = euclidean_distances(x_toexplain.to_frame().T, X)\n",
    "    #dists = pandas.Series(dists[0], index=X.index)\n",
    "    radius_all_ = dists.max()*numpy.array(radius_perc)\n",
    "\n",
    "    \n",
    "    for i in range(len(radius_all_)):\n",
    "        radius = radius_all_[i]\n",
    "        #support_x_ = numpy.array(get_random_points_hypersphere(x_toexplain, radius_=radius, n_points_=1000))\n",
    "        support_x_ = generate_inside_ball(numpy.array(x_toexplain), segment=(0, radius), n=1000)\n",
    "        \n",
    "\n",
    "        pap_fid = model_eval(sess, x, y, preds_sub, support_x_, bb_model.predict(support_x_) , args=eval_params)\n",
    "        papernot[radius_perc[i]].append(pap_fid)\n",
    "\n",
    "        ls_fid = accuracy_score(train_sub_ls.predict(support_x_), pred(support_x_))\n",
    "        localsurrogate[radius_perc[i]].append(ls_fid)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp\n",
    "imp.reload(lad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_localsurr = pandas.DataFrame(localsurrogate)\n",
    "out_papernot = pandas.DataFrame(papernot)\n",
    "\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.pointplot(data=out_papernot)\n",
    "sns.pointplot(data=out_localsurr, color='orange')\n",
    "plt.xlabel('Radius percent')\n",
    "plt.ylabel('Local Accuracy')\n",
    "plt.savefig('figures/local_fidelity_german.pdf')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_papernot.to_csv('aze.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "def sq(x):\n",
    "    return sq[0] + sq[1] / sq[0] + sq[1]\n",
    "\n",
    "with Pool(5) as p:\n",
    "    print(p.map(sq, [xs_toexplain]))\n",
    "    \n",
    "sum(xs_toexplain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
