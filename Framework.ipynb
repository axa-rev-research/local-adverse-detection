{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\"\"\"\n",
    "This tutorial shows how to generate adversarial examples\n",
    "using FGSM in black-box setting.\n",
    "The original paper can be found at:\n",
    "https://arxiv.org/abs/1602.02697\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import xrange\n",
    "\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.platform import flags\n",
    "\n",
    "from cleverhans.utils_mnist import data_mnist\n",
    "from cleverhans.utils import to_categorical\n",
    "from cleverhans.utils import set_log_level\n",
    "from cleverhans.utils_tf import model_train, model_eval, batch_eval\n",
    "from cleverhans.attacks import FastGradientMethod\n",
    "from cleverhans.attacks_tf import jacobian_graph, jacobian_augmentation\n",
    "\n",
    "from cleverhans_tutorials.tutorial_models import make_basic_cnn, MLP\n",
    "from cleverhans_tutorials.tutorial_models import Flatten, Linear, ReLU, Softmax\n",
    "from cleverhans.utils import TemporaryLogLevel\n",
    "\n",
    "from lad import lad\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "MOONS\n",
    "'''\n",
    "def get_moon():\n",
    "    X, y = make_moons(noise=0.3, random_state=1, n_samples=10000)\n",
    "    y2 = np.zeros((X.shape[0],2))\n",
    "    for k in range(len(y)):\n",
    "        y2[k][y[k]] = 1\n",
    "    return X, y2\n",
    "DATASETS_ = {'moons':get_moon}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a black-box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "PAPERNOT BB\n",
    "'''\n",
    "def Papernot_bbox(sess, x, y, X_train, Y_train, X_test, Y_test,\n",
    "              nb_epochs, batch_size, learning_rate,\n",
    "              rng):\n",
    "    \"\"\"\n",
    "    Define and train a model that simulates the \"remote\"\n",
    "    black-box oracle described in the original paper.\n",
    "    :param sess: the TF session\n",
    "    :param x: the input placeholder for MNIST\n",
    "    :param y: the ouput placeholder for MNIST\n",
    "    :param X_train: the training data for the oracle\n",
    "    :param Y_train: the training labels for the oracle\n",
    "    :param X_test: the testing data for the oracle\n",
    "    :param Y_test: the testing labels for the oracle\n",
    "    :param nb_epochs: number of epochs to train model\n",
    "    :param batch_size: size of training batches\n",
    "    :param learning_rate: learning rate for training\n",
    "    :param rng: numpy.random.RandomState\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # Define TF model graph (for the black-box model)\n",
    "    model = make_basic_cnn()\n",
    "    predictions = model(x)\n",
    "    print(\"Defined TensorFlow model graph.\")\n",
    "\n",
    "    # Train an MNIST model\n",
    "    train_params = {\n",
    "        'nb_epochs': nb_epochs,\n",
    "        'batch_size': batch_size,\n",
    "        'learning_rate': learning_rate\n",
    "    }\n",
    "    model_train(sess, x, y, predictions, X_train, Y_train,\n",
    "                args=train_params, rng=rng)\n",
    "\n",
    "    # Print out the accuracy on legitimate data\n",
    "    eval_params = {'batch_size': batch_size}\n",
    "    accuracy = model_eval(sess, x, y, predictions, X_test, Y_test,\n",
    "                          args=eval_params)\n",
    "    print('Test accuracy of black-box on legitimate test '\n",
    "          'examples: ' + str(accuracy))\n",
    "\n",
    "    return model, predictions, accuracy\n",
    "\n",
    "def RF_bbox(X_train, Y_train, X_test, Y_test):\n",
    "    # Define RF model graph (for the black-box model)\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators=100, n_jobs=-1).fit(X_train, Y_train)\n",
    "    \n",
    "    # Print out the accuracy on legitimate data\n",
    "    #predictions = model.predict_proba(X_test)[1] TEST CHANGER PREDICTIONS > FONCTION\n",
    "    predictions=lambda x: model.predict_proba(x)[1] #predict_proba required ou alors changer du code (argmax et compagnie) de papernot\n",
    "    \n",
    "    accuracy = accuracy_score(Y_test, model.predict(X_test))\n",
    "    #roc_auc = roc_auc_score(Y_test, predictions[1][:,1])\n",
    "    print('Test accuracy of black-box on legitimate test '\n",
    "          'examples: ' + str(accuracy))\n",
    "    #print('Test ROC AUC of black-box on legitimate test ' 'examples: ' + str(roc_auc))\n",
    "        \n",
    "    \n",
    "    return model, predictions, accuracy\n",
    "    \n",
    "BB_MODELS_ = {'dnn': Papernot_bbox,\n",
    "            'rf': RF_bbox}\n",
    "#ne pas utiliser dnn ca marche pas pour le moment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Papernot Surrogate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_tutorial():\n",
    "    \"\"\"\n",
    "    Helper function to check correct configuration of tf for tutorial\n",
    "    :return: True if setup checks completed\n",
    "    \"\"\"\n",
    "\n",
    "    # Set TF random seed to improve reproducibility\n",
    "    tf.set_random_seed(1234)\n",
    "\n",
    "    return True\n",
    "def substitute_model(img_rows=1, img_cols=2, nb_classes=2):\n",
    "    \"\"\"\n",
    "    Defines the model architecture to be used by the substitute. Use\n",
    "    the example model interface.\n",
    "    :param img_rows: number of rows in input\n",
    "    :param img_cols: number of columns in input\n",
    "    :param nb_classes: number of classes in output\n",
    "    :return: tensorflow model\n",
    "    \"\"\"\n",
    "    input_shape = (None, img_rows, img_cols, 1) #on garde format d'origine parce qu'on comprend pas grand chose mais on change valeurs\n",
    "\n",
    "    # Define a fully connected model (it's different than the black-box)\n",
    "    '''layers = [Flatten(),\n",
    "              Linear(200),\n",
    "              ReLU(),\n",
    "              Linear(200),\n",
    "              ReLU(),\n",
    "              Linear(nb_classes),\n",
    "              Softmax()]'''\n",
    "    layers = [Flatten(), Linear(nb_classes), Softmax()] #surrogate simplifié\n",
    "\n",
    "    return MLP(layers, input_shape)\n",
    "\n",
    "\n",
    "def train_sub(sess, x, y, bbox_preds, X_sub, Y_sub, nb_classes,\n",
    "              nb_epochs_s, batch_size, learning_rate, data_aug, lmbda,\n",
    "              rng):\n",
    "    \"\"\"\n",
    "    This function creates the substitute by alternatively\n",
    "    augmenting the training data and training the substitute.\n",
    "    :param sess: TF session\n",
    "    :param x: input TF placeholder\n",
    "    :param y: output TF placeholder\n",
    "    :param bbox_preds: output of black-box model predictions\n",
    "    :param X_sub: initial substitute training data\n",
    "    :param Y_sub: initial substitute training labels\n",
    "    :param nb_classes: number of output classes\n",
    "    :param nb_epochs_s: number of epochs to train substitute model\n",
    "    :param batch_size: size of training batches\n",
    "    :param learning_rate: learning rate for training\n",
    "    :param data_aug: number of times substitute training data is augmented\n",
    "    :param lmbda: lambda from arxiv.org/abs/1602.02697\n",
    "    :param rng: numpy.random.RandomState instance\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Define TF model graph (for the black-box model)\n",
    "    model_sub = substitute_model()\n",
    "    preds_sub = model_sub(x)\n",
    "    print(\"Defined TensorFlow model graph for the substitute.\")\n",
    "\n",
    "    # Define the Jacobian symbolically using TensorFlow\n",
    "    grads = jacobian_graph(preds_sub, x, nb_classes)\n",
    "    # Train the substitute and augment dataset alternatively\n",
    "    for rho in xrange(data_aug):\n",
    "        print(\"Substitute training epoch #\" + str(rho))\n",
    "        train_params = {\n",
    "            'nb_epochs': nb_epochs_s,\n",
    "            'batch_size': batch_size,\n",
    "            'learning_rate': learning_rate\n",
    "        }\n",
    "        with TemporaryLogLevel(logging.WARNING, \"cleverhans.utils.tf\"):\n",
    "            model_train(sess, x, y, preds_sub, X_sub,\n",
    "                        to_categorical(Y_sub, nb_classes),\n",
    "                        init_all=False, args=train_params, rng=rng)\n",
    "\n",
    "        # If we are not at last substitute training iteration, augment dataset\n",
    "        if rho < data_aug - 1:\n",
    "            print(\"Augmenting substitute training data.\")\n",
    "            # Perform the Jacobian augmentation\n",
    "            lmbda_coef = 2 * int(int(rho / 3) != 0) - 1\n",
    "            print('a')\n",
    "            X_sub = jacobian_augmentation(sess, x, X_sub, Y_sub, grads,\n",
    "                                          lmbda_coef * lmbda)\n",
    "            print('b')\n",
    "            \n",
    "            print(\"Labeling substitute training data.\")\n",
    "            # Label the newly generated synthetic points using the black-box\n",
    "            Y_sub = np.hstack([Y_sub, Y_sub])\n",
    "            X_sub_prev = X_sub[int(len(X_sub)/2):] #on a double le dataset donc prev = ce qu'il y a de nouveau = la moitie\n",
    "            eval_params = {'batch_size': batch_size}\n",
    "            \n",
    "            #bbox_preds = tf.convert_to_tensor(bbox_preds, dtype=tf.float32) TEST CHANGER PREDICTIONS > FONCTION           \n",
    "            #bbox_val = batch_eval2(sess, [x], [bbox_preds], [X_sub_prev], args=eval_params)[0] TEST CHANGER PREDICTIONS > FONCTION\n",
    "            bbox_val = bbox_preds(X_sub_prev) #normalement batch eval sert juste à sortir les preds...?\n",
    "            # Note here that we take the argmax because the adversary\n",
    "            # only has access to the label (not the probabilities) output\n",
    "            # by the black-box model\n",
    "            Y_sub[int(len(X_sub)/2):] = np.argmax(bbox_val, axis=1)\n",
    "    return model_sub, preds_sub "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "PAS UTILISE DANS LA VERSION ACTUELLE\n",
    "MAIS LAISSER AU CAS OU SVP\n",
    "'''\n",
    "\n",
    "\n",
    "def batch_eval2(sess, tf_inputs, tf_outputs, numpy_inputs, feed=None,\n",
    "               args=None):\n",
    "    #print('tf outputs', tf_outputs[0].shape) TEST CHANGER PREDICTIONS > FONCTION\n",
    "    \"\"\"\n",
    "    A helper function that computes a tensor on numpy inputs by batches.\n",
    "    :param sess:\n",
    "    :param tf_inputs:\n",
    "    :param tf_outputs:\n",
    "    :param numpy_inputs:\n",
    "    :param feed: An optional dictionary that is appended to the feeding\n",
    "             dictionary before the session runs. Can be used to feed\n",
    "             the learning phase of a Keras model for instance.\n",
    "    :param args: dict or argparse `Namespace` object.\n",
    "                 Should contain `batch_size`\n",
    "    \"\"\"\n",
    "    args = _ArgsWrapper(args or {})\n",
    "\n",
    "    assert args.batch_size, \"Batch size was not given in args dict\"\n",
    "\n",
    "    n = len(numpy_inputs)\n",
    "    assert n > 0\n",
    "    assert n == len(tf_inputs)\n",
    "    m = numpy_inputs[0].shape[0]\n",
    "    for i in xrange(1, n):\n",
    "        assert numpy_inputs[i].shape[0] == m\n",
    "    out = []\n",
    "    for _ in tf_outputs:\n",
    "        out.append([])\n",
    "    with sess.as_default():\n",
    "        for start in xrange(0, m, args.batch_size):\n",
    "            batch = start // args.batch_size\n",
    "            if batch % 100 == 0 and batch > 0:\n",
    "                _logger.debug(\"Batch \" + str(batch))\n",
    "\n",
    "            # Compute batch start and end indices\n",
    "            start = batch * args.batch_size            \n",
    "            numpy_input_batches = [numpy_input[start:end]\n",
    "                                   for numpy_input in numpy_inputs]\n",
    "            cur_batch_size = numpy_input_batches[0].shape[0]\n",
    "            assert cur_batch_size <= args.batch_size\n",
    "            \n",
    "            for e in numpy_input_batches:\n",
    "                assert e.shape[0] == cur_batch_size\n",
    "\n",
    "            feed_dict = dict(zip(tf_inputs, numpy_input_batches))\n",
    "            \n",
    "            \n",
    "            #print('feed_dict', feed_dict)\n",
    "            print(\"feed\",feed)\n",
    "            if feed is not None:\n",
    "                feed_dict.update(feed)\n",
    "            #numpy_output_batches = sess.run(tf_outputs, feed_dict=feed_dict) #PROBLEME ICI : AU LIEU DES BATCHES, TOUT LE DATASET DE PRED\n",
    "            #ici cest retourner les predictions des batchs.\n",
    "            # donc liste de pred(batchs)\n",
    "            for batch in numpy_input_batches: #TEST CHANGER PREDICTIONS > FONCTION\n",
    "                numpy_output_batches.append(tf_outputs(batch))\n",
    "            print(\"len np output bitches\", len(numpy_output_batches))\n",
    "            for e in numpy_output_batches:\n",
    "                print(\"e\", e)\n",
    "                print(\"e shape\", e.shape)\n",
    "                print(\" cur batch size useless\", cur_batch_size)\n",
    "                assert e.shape[0] == cur_batch_size, e.shape #ERREUR ICI\n",
    "            for out_elem, numpy_output_batch in zip(out, numpy_output_batches):\n",
    "                out_elem.append(numpy_output_batch)\n",
    "\n",
    "    out = [np.concatenate(x, axis=0) for x in out]\n",
    "    for e in out:\n",
    "        assert e.shape[0] == m, e.shape\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class _ArgsWrapper(object):\n",
    "\n",
    "    \"\"\"\n",
    "    Wrapper that allows attribute access to dictionaries\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args):\n",
    "        if not isinstance(args, dict):\n",
    "            args = vars(args)\n",
    "        self.args = args\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        return self.args.get(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usage: \n",
    "print(\"Training the substitute model.\")\n",
    "    train_sub_out = train_sub(sess, x, y, bbox_preds, X_sub, Y_sub,\n",
    "                              nb_classes, nb_epochs_s, batch_size,\n",
    "                              learning_rate, data_aug, lmbda, rng=rng)\n",
    "    model_sub, preds_sub = train_sub_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our surrogate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_points_hypersphere(x_center, radius_, n_points_):\n",
    "\n",
    "        res = []\n",
    "        while len(res) < n_points_:\n",
    "        \n",
    "            n_points_left_ = n_points_ - len(res)\n",
    "            # About half the points are lost in the test hypercube => hypersphere\n",
    "            lbound = numpy.repeat([x_center.values-(radius_/2.)], n_points_left_*2, axis=0)\n",
    "            hbound = numpy.repeat([x_center.values+(radius_/2.)], n_points_left_*2, axis=0)\n",
    "            points = numpy.random.uniform(low=lbound, high=hbound)\n",
    "            # Check if x_generated is within hypersphere (if kind=='hypersphere')\n",
    "            for x_generated in points:\n",
    "                if euclidean(x_generated, x_center.values) < radius_:\n",
    "                    res.append(x_generated)\n",
    "                if len(res) == n_points_:\n",
    "                    break\n",
    "\n",
    "        return pandas.DataFrame(numpy.array(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training black box on 8000 examples\n",
      "Testing black box and substitute on 1850  examples\n",
      "Using  150  examples to start PP substitute\n",
      "Preparing the black-box model.\n",
      "Test accuracy of black-box on legitimate test examples: 0.9059459459459459\n",
      "Training the Pépèrenot substitute model.\n",
      "Defined TensorFlow model graph for the substitute.\n",
      "Substitute training epoch #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 2018-06-26 15:34:57,914 cleverhans] Epoch 0 took 0.019896507263183594 seconds\n",
      "[INFO 2018-06-26 15:34:57,915 cleverhans] Completed model training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting substitute training data.\n",
      "a\n",
      "b\n",
      "Labeling substitute training data.\n",
      "Substitute training epoch #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 2018-06-26 15:35:01,660 cleverhans] Epoch 0 took 0.0197751522064209 seconds\n",
      "[INFO 2018-06-26 15:35:01,661 cleverhans] Completed model training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting substitute training data.\n",
      "a\n",
      "b\n",
      "Labeling substitute training data.\n",
      "Substitute training epoch #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 2018-06-26 15:35:11,152 cleverhans] Epoch 0 took 0.022075414657592773 seconds\n",
      "[INFO 2018-06-26 15:35:11,153 cleverhans] Completed model training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Local Surrogate substitute model.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'localsurrogate': 1.0, 'papernot': 0.0},\n",
       " {'bbox': 0.9059459459459459, 'papernot': 0.21027027027027026})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def main_fidelity():\n",
    "    accuracies = {}\n",
    "    fidelities = {}\n",
    "    \n",
    "    \n",
    "    # Seed random number generator so tutorial is reproducible\n",
    "    rng = np.random.RandomState([2017, 8, 30])\n",
    "\n",
    "    # Thibault: Tensorflow stuff\n",
    "    set_log_level(logging.DEBUG)\n",
    "    assert setup_tutorial()\n",
    "    sess = tf.Session()\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Data\n",
    "    X, Y = DATASETS_['moons']()\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20)\n",
    "    X_sub = X_test[:holdout]\n",
    "    Y_sub = np.argmax(Y_test[:holdout], axis=1)\n",
    "\n",
    "    ## Redefine test set as remaining samples unavailable to adversaries\n",
    "    ### N.B Thibault: c'est pour le substitute de Papernot\n",
    "    X_test = X_test[holdout:]\n",
    "    Y_test = Y_test[holdout:]\n",
    "    print(\"Training black box on\",X_train.shape[0], \"examples\")\n",
    "    print('Testing black box and substitute on', X_test.shape[0],' examples')\n",
    "    print(\"Using \", holdout, \" examples to start PP substitute\")\n",
    "    ## Define input and output TF placeholders\n",
    "    ### N.B. Thibault: restes de Tensorflow, utilisé pour le substitute de Papernot...\n",
    "    x = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "    y = tf.placeholder(tf.float32, shape=(None, 2))  \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Instance to explain\n",
    "    x_toexplain = pandas.Series(X_test[0]).copy()\n",
    "    support_x_ = np.array(get_random_points_hypersphere(x_toexplain, radius_=radius, n_points_=1000))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Simulate the black-box model\n",
    "    print(\"Preparing the black-box model.\")\n",
    "    prep_bbox_out = BB_MODELS_['rf'](X_train, Y_train, X_test, Y_test)\n",
    "    bb_model, bbox_preds, accuracies['bbox'] = prep_bbox_out #bbox_preds fonction predict\n",
    "    \n",
    "    # Train PAPERNOT substitute\n",
    "    print(\"Training the Pépèrenot substitute model.\")\n",
    "    train_sub_pap = train_sub(sess, x, y, bbox_preds, X_sub, Y_sub,\n",
    "                              nb_classes, nb_epochs_s, batch_size,\n",
    "                              learning_rate, data_aug, lmbda, rng=rng)\n",
    "    model_sub, preds_sub = train_sub_pap\n",
    "    eval_params = {'batch_size': batch_size}\n",
    "    pap_acc = model_eval(sess, x, y, preds_sub, X_test, Y_test, args=eval_params) \n",
    "    pap_fid = model_eval(sess, x, y, preds_sub, support_x_, bbox_preds(support_x_) , args=eval_params)\n",
    "    accuracies['papernot'] = pap_acc\n",
    "    fidelities['papernot'] = pap_fid\n",
    "    \n",
    "    \n",
    "    # Train OUR subtitute\n",
    "    print(\"Training Local Surrogate substitute model.\")\n",
    "    pred = bb_model.predict\n",
    "    bb_model.predict = lambda x: pred(x)[:,1]\n",
    "    _, train_sub_ls = lad.LocalSurrogate(pandas.DataFrame(X), blackbox=bb_model, n_support_points=100, max_depth=3).get_local_surrogate(x_toexplain)\n",
    "    #ls_acc = accuracy_score(train_sub_ls.predict(X_test), Y_test)\n",
    "    ls_fid = accuracy_score(train_sub_ls.predict(support_x_), bb_model.predict(support_x_))\n",
    "    #accuracies['localsurrogate'] = ls_acc\n",
    "    fidelities['localsurrogate'] = ls_fid\n",
    "    \n",
    "\n",
    "    \n",
    "    return fidelities, accuracies\n",
    "\n",
    "\n",
    "\n",
    "nb_classes=2 #\n",
    "batch_size=120 #\n",
    "learning_rate=0.001 #\n",
    "nb_epochs=1 # Nombre d'itération bbox osef\n",
    "holdout=150 # Nombre d'exemples utilisés au début pour générer data (Pap-substitute)\n",
    "data_aug=3 # Nombre d'itérations d'augmentation du dataset {IMPORTANT pour Pap-substitute}\n",
    "nb_epochs_s=1 # Nombre d'itérations pour train substitute\n",
    "lmbda=0.1 # params exploration pour augmentation data\n",
    "radius = 0.5 # NEW\n",
    "\n",
    "\n",
    "main_fidelity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas.Series(X[0])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = DATASETS_['moons']()\n",
    "X.min(axis=0), X[:,1].min()\n",
    "\n",
    "\n",
    "support_points = []\n",
    "\n",
    "while len(support_points) < 10:\n",
    "    print(len(support_points))\n",
    "    candidate_ = numpy.random.uniform(low=X.min(axis=0),\n",
    "                               high=X.max(axis=0))\n",
    "    if self.blackbox.predict([candidate_]).all() != self.blackbox.predict([x_toexplain]).all():\n",
    "        support_points.append(candidate_)\n",
    "\n",
    "support_points = pandas.DataFrame(support_points, columns=x_toexplain.index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
